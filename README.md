# ğŸ“Š ANBIMA Data Scraper - DocumentaÃ§Ã£o Completa

Sistema automatizado para extraÃ§Ã£o de dados periÃ³dicos de fundos de investimento do site da ANBIMA.

---

## ğŸ“‘ Ãndice

1. [VisÃ£o Geral](#-visÃ£o-geral)
2. [Requisitos do Sistema](#-requisitos-do-sistema)
3. [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
4. [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
5. [Guia de Uso](#-guia-de-uso)
6. [Arquitetura do Sistema](#-arquitetura-do-sistema)
7. [EspecificaÃ§Ãµes TÃ©cnicas](#-especificaÃ§Ãµes-tÃ©cnicas)
8. [Dados ExtraÃ­dos](#-dados-extraÃ­dos)
9. [Tratamento de Erros](#-tratamento-de-erros)
10. [SoluÃ§Ã£o de Problemas](#-soluÃ§Ã£o-de-problemas)
11. [Perguntas Frequentes](#-perguntas-frequentes)
12. [LimitaÃ§Ãµes Conhecidas](#-limitaÃ§Ãµes-conhecidas)
13. [ManutenÃ§Ã£o e AtualizaÃ§Ãµes](#-manutenÃ§Ã£o-e-atualizaÃ§Ãµes)

---

## ğŸ¯ VisÃ£o Geral

### DescriÃ§Ã£o do Projeto

O **ANBIMA Data Scraper** Ã© uma soluÃ§Ã£o automatizada desenvolvida em Python para extraÃ§Ã£o de dados histÃ³ricos de fundos de investimento disponÃ­veis publicamente no portal [ANBIMA Data](https://data.anbima.com.br/busca/fundos).

### Objetivo

Automatizar o processo de coleta de dados periÃ³dicos (Data da cotizaÃ§Ã£o e Valor da cota) de mÃºltiplos fundos de investimento, eliminando a necessidade de consultas manuais e facilitando a anÃ¡lise histÃ³rica de dados.

### Principais CaracterÃ­sticas

- âœ… **AutomaÃ§Ã£o Completa**: Processo end-to-end desde a leitura de CNPJs atÃ© a geraÃ§Ã£o do arquivo Excel
- âœ… **ExtraÃ§Ã£o Seletiva**: Coleta apenas os campos especÃ­ficos solicitados (Data e Valor da cota)
- âœ… **OrganizaÃ§Ã£o CronolÃ³gica**: Dados ordenados automaticamente do mais antigo ao mais recente
- âœ… **Robustez**: Sistema de retry automÃ¡tico para requisiÃ§Ãµes que falham
- âœ… **Rastreabilidade**: Logs detalhados de todas as operaÃ§Ãµes
- âœ… **Interface AmigÃ¡vel**: Barra de progresso e relatÃ³rios de execuÃ§Ã£o
- âœ… **ConfigurÃ¡vel**: ParÃ¢metros ajustÃ¡veis para diferentes cenÃ¡rios de uso
- âš¡ **NOVO: Modo Paralelo**: Processa mÃºltiplos CNPJs simultaneamente (atÃ© 75% mais rÃ¡pido!)

### Fluxo de Trabalho

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  input_cnpjs    â”‚
â”‚   .xlsx         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  main.py        â”‚
â”‚  (Orquestrador) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ anbima_scraper  â”‚â”€â”€â”€â”€â–¶â”‚  Site ANBIMA     â”‚
â”‚   .py           â”‚     â”‚  (Selenium)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data_processor  â”‚
â”‚   .py           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  output_*.xlsx  â”‚     â”‚  logs/*.log      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Requisitos do Sistema

### Hardware MÃ­nimo

- **Processador**: 1 GHz ou superior
- **MemÃ³ria RAM**: 2 GB (recomendado: 4 GB ou mais)
- **EspaÃ§o em Disco**: 500 MB livres
- **ConexÃ£o Ã  Internet**: EstÃ¡vel, velocidade mÃ­nima de 1 Mbps

### Software NecessÃ¡rio

| Software | VersÃ£o MÃ­nima | VersÃ£o Recomendada | ObservaÃ§Ãµes |
|----------|---------------|-------------------|-------------|
| Python | 3.9 | 3.10 ou superior | ObrigatÃ³rio |
| Google Chrome | Ãšltima versÃ£o estÃ¡vel | Ãšltima versÃ£o | ObrigatÃ³rio |
| Sistema Operacional | Windows 10 / macOS 10.14 / Ubuntu 18.04 | VersÃµes mais recentes | - |

### DependÃªncias Python

Todas as dependÃªncias sÃ£o instaladas automaticamente via `requirements.txt`:

```
selenium==4.15.2        # AutomaÃ§Ã£o do navegador
pandas==2.1.3           # ManipulaÃ§Ã£o de dados tabulares
openpyxl==3.1.2         # Leitura/escrita de arquivos Excel
webdriver-manager==4.0.1 # Gerenciamento automÃ¡tico do ChromeDriver
tqdm==4.66.1            # Barra de progresso
```

---

## ğŸ”§ InstalaÃ§Ã£o

### InstalaÃ§Ã£o RÃ¡pida

1. **Clone ou faÃ§a download do projeto**

2. **Navegue atÃ© o diretÃ³rio do projeto**
```bash
cd "/Users/LuizPersechini_1/Projects/Eduardo Scrapping"
```

3. **Instale as dependÃªncias**
```bash
pip3 install -r requirements.txt
```

### InstalaÃ§Ã£o Detalhada

#### Passo 1: Verificar Python

```bash
python3 --version
```

Se nÃ£o tiver Python instalado, baixe em [python.org](https://www.python.org/downloads/)

#### Passo 2: Verificar Google Chrome

O Google Chrome deve estar instalado no sistema. Baixe em [google.com/chrome](https://www.google.com/chrome/)

#### Passo 3: Criar Ambiente Virtual (Opcional mas Recomendado)

```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate  # Windows
```

#### Passo 4: Instalar DependÃªncias

```bash
pip3 install -r requirements.txt
```

#### Passo 5: Verificar InstalaÃ§Ã£o

```bash
python3 -c "import selenium, pandas, openpyxl; print('âœ“ InstalaÃ§Ã£o bem-sucedida!')"
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo de ConfiguraÃ§Ã£o (`config.py`)

O arquivo `config.py` contÃ©m todas as configuraÃ§Ãµes do sistema:

#### URLs e Endpoints

```python
ANBIMA_BASE_URL = "https://data.anbima.com.br/busca/fundos"
```

#### Timeouts (em segundos)

```python
PAGE_LOAD_TIMEOUT = 30      # Tempo mÃ¡ximo para carregar pÃ¡gina
ELEMENT_WAIT_TIMEOUT = 20   # Tempo mÃ¡ximo para encontrar elementos
IMPLICIT_WAIT = 10          # Espera implÃ­cita entre aÃ§Ãµes
SLEEP_BETWEEN_REQUESTS = 2  # Delay entre requisiÃ§Ãµes
```

**Quando ajustar:**
- **ConexÃ£o lenta**: Aumente todos os timeouts em 50%
- **ConexÃ£o rÃ¡pida**: Pode reduzir para acelerar o processo
- **Site sobrecarregado**: Aumente `SLEEP_BETWEEN_REQUESTS`

#### Sistema de Retry

```python
MAX_RETRIES = 3     # NÃºmero de tentativas em caso de falha
RETRY_DELAY = 5     # Segundos entre tentativas
```

#### OpÃ§Ãµes do Chrome

```python
CHROME_OPTIONS = [
    "--headless",                          # ExecuÃ§Ã£o em background
    "--no-sandbox",                        # SeguranÃ§a
    "--disable-dev-shm-usage",            # Uso de memÃ³ria
    "--disable-gpu",                       # GPU
    "--window-size=1920,1080",            # ResoluÃ§Ã£o
    "--disable-blink-features=AutomationControlled",  # Anti-detecÃ§Ã£o
    "user-agent=Mozilla/5.0..."           # User agent
]
```

#### Estrutura de Dados

```python
OUTPUT_COLUMNS = [
    "CNPJ",
    "Nome do Fundo",
    "Data da cotizaÃ§Ã£o",
    "Valor cota",
    "Status"
]

INPUT_COLUMN_CNPJ = "CNPJ"
```

### Arquivo de Entrada (`input_cnpjs.xlsx`)

#### Formato Esperado

| CNPJ |
|------|
| 48.330.198/0001-06 |
| 34.780.531/0001-66 |
| ... |

#### Requisitos

- **Nome da coluna**: Exatamente "CNPJ" (case-sensitive)
- **Formato do CNPJ**: Com ou sem formataÃ§Ã£o (pontos, barras)
- **ExtensÃ£o**: `.xlsx` (Excel 2007 ou superior)
- **LocalizaÃ§Ã£o**: Raiz do projeto (ou especifique com `-i`)

#### Exemplo de CriaÃ§Ã£o Manual

```python
import pandas as pd

cnpjs = ["48.330.198/0001-06", "34.780.531/0001-66"]
df = pd.DataFrame({"CNPJ": cnpjs})
df.to_excel("input_cnpjs.xlsx", index=False)
```

---

## ğŸ“– Guia de Uso

### Uso BÃ¡sico

#### 1. Modo PadrÃ£o (Headless)

```bash
python3 main.py
```

**CaracterÃ­sticas:**
- Navegador invisÃ­vel (background)
- Mais rÃ¡pido
- Ideal para produÃ§Ã£o

#### 2. Modo VisÃ­vel (Debug)

```bash
python3 main.py --no-headless
```

**CaracterÃ­sticas:**
- Navegador visÃ­vel
- Permite observar o processo
- Ideal para debug e testes

#### 3. Personalizar Arquivos

```bash
python3 main.py -i meus_fundos.xlsx -o resultados_outubro.xlsx
```

### âš¡ Modo Paralelo (NOVO!)

O modo paralelo permite processar mÃºltiplos CNPJs simultaneamente, reduzindo o tempo total em atÃ© **75%**!

#### 1. Modo Paralelo PadrÃ£o (4 workers)

```bash
python3 main_parallel.py
```

**CaracterÃ­sticas:**
- Processa 4 CNPJs simultaneamente
- ~75% mais rÃ¡pido que o modo sequencial
- Ideal para listas grandes de CNPJs

#### 2. Modo Paralelo Personalizado

```bash
python3 main_parallel.py -i input_cnpjs.xlsx -o output.xlsx --workers 6
```

**OpÃ§Ãµes disponÃ­veis:**
- `-w, --workers N`: NÃºmero de workers (padrÃ£o: 4, mÃ¡ximo recomendado: 8)
- `--skip-processed`: Pula CNPJs jÃ¡ processados no arquivo de saÃ­da

#### 3. Modo Paralelo com Skip

```bash
python3 main_parallel.py --skip-processed
```

**Ãštil para:**
- Continuar uma execuÃ§Ã£o interrompida
- Adicionar novos CNPJs sem reprocessar os antigos
- Economizar tempo em re-execuÃ§Ãµes

#### ComparaÃ§Ã£o de Performance

| CenÃ¡rio | Modo Sequencial | Modo Paralelo (4 workers) | Modo Paralelo (5 workers) | Melhor Ganho |
|---------|----------------|---------------------------|---------------------------|--------------|
| 10 CNPJs | ~11 minutos | ~3 minutos | ~2 minutos | **82%** |
| 50 CNPJs | ~57 minutos | ~15 minutos | ~8 minutos | **86%** |
| 161 CNPJs | ~3h 25min | ~37 minutos | **27 minutos** â­ | **87%** |

**Tempo mÃ©dio por CNPJ:**
- Sequencial: ~68 segundos
- Paralelo (4 workers): ~17 segundos por conjunto
- Paralelo (5 workers): **~10 segundos por conjunto** ğŸš€

**Resultados Reais (161 CNPJs)**:
- 5 workers: 27.23 minutos
- Taxa de sucesso: 95.7%
- Throughput: 354.71 CNPJs/hora

#### Quando Usar Cada Modo

| SituaÃ§Ã£o | Modo Recomendado |
|----------|------------------|
| Primeira execuÃ§Ã£o com muitos CNPJs (100+) | **Paralelo 5 workers** â­ |
| Listas mÃ©dias (50-100 CNPJs) | Paralelo 4 workers |
| Listas pequenas (20-50 CNPJs) | Paralelo 2 workers |
| Debug ou desenvolvimento | Sequencial (--no-headless) |
| Poucos CNPJs (< 20) | Sequencial |
| Re-execuÃ§Ã£o mensal (lista completa) | **Paralelo 5 workers** (--skip-processed) |
| Sistema com recursos limitados | Paralelo 2 workers |
| Teste de funcionalidade | Sequencial |

### Uso AvanÃ§ado

#### Executar com ParÃ¢metros Customizados

```bash
python3 main.py \
  --input fundos_renda_fixa.xlsx \
  --output relatorio_$(date +%Y%m%d).xlsx \
  --no-headless
```

#### Executar em Background (Linux/macOS)

```bash
nohup python3 main.py > execucao.log 2>&1 &
```

#### Agendar ExecuÃ§Ã£o (Cron - Linux/macOS)

```bash
# Editar crontab
crontab -e

# Executar todo dia Ã s 18h
0 18 * * * cd /caminho/projeto && python3 main.py
```

#### Agendar ExecuÃ§Ã£o (Task Scheduler - Windows)

1. Abrir "Agendador de Tarefas"
2. Criar Tarefa BÃ¡sica
3. Apontar para `python.exe` com argumentos: `main.py`
4. Definir diretÃ³rio de trabalho: caminho do projeto

### Interpretando a SaÃ­da

#### Durante a ExecuÃ§Ã£o

```
âœ“ Found 2 CNPJ(s) to process
âœ“ Web scraper initialized successfully

ğŸ” Scraping data for 2 fund(s)...

Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:57<00:57, 57.19s/fund]
```

#### Ao Final

```
================================================================================
SCRAPING SUMMARY
================================================================================
Total CNPJs processed: 2
Successful: 2 (100.0%)
Failed: 0

âœ“ Results saved to: output_anbima_data_20251023_110341.xlsx
âœ“ Log file saved to: logs/
================================================================================
```

### Arquivo de SaÃ­da

#### Estrutura

```
output_anbima_data_YYYYMMDD_HHMMSS.xlsx
```

#### ConteÃºdo

| CNPJ | Nome do Fundo | Data da cotizaÃ§Ã£o | Valor cota | Status |
|------|---------------|-------------------|------------|--------|
| 48.330.198/0001-06 | CLASSE ÃšNICA... | 19/09/2025 | R$ 1,569379 | Success |
| 48.330.198/0001-06 | CLASSE ÃšNICA... | 22/09/2025 | R$ 1,570331 | Success |

---

## ğŸ—ï¸ Arquitetura do Sistema

### Componentes Principais

#### 1. `main.py` - Orquestrador

**Responsabilidades:**
- InicializaÃ§Ã£o do sistema
- Leitura de parÃ¢metros CLI
- CoordenaÃ§Ã£o entre mÃ³dulos
- Tratamento de exceÃ§Ãµes globais
- GeraÃ§Ã£o de relatÃ³rios

**FunÃ§Ãµes Principais:**
- `setup_logging()`: Configura sistema de logs
- `main()`: FunÃ§Ã£o principal de execuÃ§Ã£o

#### 2. `anbima_scraper.py` - Motor de Scraping

**Responsabilidades:**
- AutomaÃ§Ã£o do navegador via Selenium
- NavegaÃ§Ã£o no site da ANBIMA
- ExtraÃ§Ã£o de dados das pÃ¡ginas
- Tratamento de erros de navegaÃ§Ã£o

**Classe Principal: `ANBIMAScraper`**

```python
class ANBIMAScraper:
    def setup_driver(self) -> bool
    def search_fund(self, cnpj: str) -> Tuple[bool, str]
    def get_fund_name(self) -> Optional[str]
    def navigate_to_periodic_data(self) -> Tuple[bool, str]
    def extract_periodic_data(self) -> Tuple[bool, List[Dict], str]
    def scrape_fund_data(self, cnpj: str) -> Dict
    def close(self)
```

#### 3. `data_processor.py` - Processador de Dados

**Responsabilidades:**
- Leitura do arquivo Excel de entrada
- TransformaÃ§Ã£o e limpeza de dados
- ExportaÃ§Ã£o para Excel
- GeraÃ§Ã£o de relatÃ³rios de resumo

**Classe Principal: `DataProcessor`**

```python
class DataProcessor:
    def read_cnpj_list(self, input_file: str) -> List[str]
    def process_scraped_data(self, results: List[Dict]) -> pd.DataFrame
    def save_results(self, df: pd.DataFrame, output_file: str)
    def create_summary_report(self, results: List[Dict]) -> Dict
```

#### 4. `config.py` - ConfiguraÃ§Ãµes

**ConteÃºdo:**
- URLs e endpoints
- Timeouts e delays
- Seletores CSS/XPath
- Estrutura de dados
- ConfiguraÃ§Ãµes do Chrome

### Fluxo de Dados

```
1. main.py lÃª input_cnpjs.xlsx
2. Para cada CNPJ:
   a. ANBIMAScraper.search_fund()
   b. ANBIMAScraper.get_fund_name()
   c. ANBIMAScraper.navigate_to_periodic_data()
   d. ANBIMAScraper.extract_periodic_data()
3. DataProcessor.process_scraped_data()
4. DataProcessor.save_results()
5. Gera logs e relatÃ³rios
```

---

## ğŸ”¬ EspecificaÃ§Ãµes TÃ©cnicas

### Algoritmo de Scraping

#### Etapa 1: Busca do Fundo

```python
1. Acessa https://data.anbima.com.br/busca/fundos
2. Aguarda carregamento completo (sleep 3s)
3. Fecha banner de cookies se presente
4. Localiza campo de busca por seletor CSS
5. Digita CNPJ
6. Aguarda dropdown de resultados (sleep 3s)
7. Clica no link "em fundos de investimento"
8. Aguarda pÃ¡gina de resultados (sleep 2s)
9. Fecha dropdown se ainda aberto
10. Localiza e clica no primeiro resultado
```

#### Etapa 2: ExtraÃ§Ã£o de Dados

```python
1. ObtÃ©m nome do fundo da pÃ¡gina de detalhes
2. ConstrÃ³i URL de dados periÃ³dicos
3. Navega para /fundos/{CODE}/dados-periodicos
4. Aguarda carregamento da tabela (sleep 3s)
5. Identifica Ã­ndices das colunas:
   - Data competÃªncia (Ã­ndice 0)
   - Valor cota (Ã­ndice 2)
6. Executa scroll para garantir todos os dados carregados
7. Extrai dados linha por linha
8. Remove duplicatas por data
9. Inverte ordem (mais antigo primeiro)
10. Retorna lista de dicionÃ¡rios
```

### Seletores Utilizados

#### CSS Selectors

```python
"input[placeholder*='Busque fundos']"  # Campo de busca
"article a[href*='/fundos/C']"         # Link do fundo
"table"                                 # Tabela de dados
"th, td"                                # CÃ©lulas de cabeÃ§alho
```

#### XPath Selectors

```python
"//a[contains(@href, '/busca/fundos?q=')]"  # Link dropdown
"//button[contains(text(), 'Prosseguir')]"  # Aceitar cookies
```

### EstratÃ©gia de Waits

O sistema utiliza 3 tipos de waits:

1. **Explicit Wait** (WebDriverWait)
   ```python
   wait = WebDriverWait(driver, 20)
   element = wait.until(EC.presence_of_element_located((By.CSS, selector)))
   ```

2. **Implicit Wait**
   ```python
   driver.implicitly_wait(10)
   ```

3. **Sleep** (para carregamentos AJAX)
   ```python
   time.sleep(2)  # Aguarda AJAX completar
   ```

### Sistema de Retry

```python
max_retries = 3
for attempt in range(max_retries):
    try:
        result = scrape_fund_data(cnpj)
        if result['Status'] == 'Success':
            break
    except Exception:
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
        else:
            result = {'Status': 'Error after max retries'}
```

---

## ğŸ“Š Dados ExtraÃ­dos

### Campos Coletados

De acordo com as especificaÃ§Ãµes do projeto, sÃ£o extraÃ­dos **apenas** 2 campos:

| Campo | Tipo | DescriÃ§Ã£o | Exemplo |
|-------|------|-----------|---------|
| Data da cotizaÃ§Ã£o | String | Data de competÃªncia do valor da cota | 19/09/2025 |
| Valor cota | String | Valor da cota na data | R$ 1,569379 |

### Campos ExcluÃ­dos

Os seguintes campos sÃ£o **intencionalmente excluÃ­dos** da extraÃ§Ã£o:

- âŒ Valor patrimÃ´nio lÃ­quido
- âŒ Valor volume total de aplicaÃ§Ãµes
- âŒ Valor volume total de resgates
- âŒ NÃºmero total de cotistas

### Metadados Adicionados

O sistema adiciona os seguintes metadados:

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| CNPJ | String | CNPJ do fundo consultado |
| Nome do Fundo | String | Nome completo do fundo |
| Status | String | Status da extraÃ§Ã£o (Success/Error) |

### OrganizaÃ§Ã£o dos Dados

- **OrdenaÃ§Ã£o**: CronolÃ³gica ascendente (mais antigo â†’ mais recente)
- **Agrupamento**: Por CNPJ
- **Formato de SaÃ­da**: Excel (.xlsx)

### LimitaÃ§Ãµes de Dados

**Importante**: O site da ANBIMA exibe apenas os **Ãºltimos 22 dias Ãºteis** de dados periÃ³dicos. NÃ£o hÃ¡ controles de paginaÃ§Ã£o ou filtros de data disponÃ­veis na interface web.

**SoluÃ§Ã£o para histÃ³rico maior:**
- Executar o scraper periodicamente (diÃ¡rio, semanal ou mensal)
- Armazenar resultados em banco de dados
- Consolidar dados ao longo do tempo

---

## ğŸ›¡ï¸ Tratamento de Erros

### Tipos de Erros

#### 1. Erros de InicializaÃ§Ã£o

**ChromeDriver nÃ£o encontrado**
```
Erro: Failed to initialize WebDriver
Causa: Chrome nÃ£o instalado ou ChromeDriver incompatÃ­vel
SoluÃ§Ã£o: Instalar Google Chrome / Limpar cache do webdriver-manager
```

**Arquivo de entrada nÃ£o encontrado**
```
Erro: Input file not found
Causa: Arquivo input_cnpjs.xlsx nÃ£o existe
SoluÃ§Ã£o: Criar arquivo ou especificar caminho correto com -i
```

#### 2. Erros de Scraping

**Timeout de pÃ¡gina**
```
Erro: Timeout: Page took too long to load
Causa: ConexÃ£o lenta ou site indisponÃ­vel
SoluÃ§Ã£o: Aumentar timeouts em config.py / Verificar conexÃ£o
```

**CNPJ nÃ£o encontrado**
```
Erro: No fund found for this CNPJ
Causa: CNPJ nÃ£o existe na base ANBIMA ou digitado incorretamente
SoluÃ§Ã£o: Verificar CNPJ no site manualmente
```

**Elementos nÃ£o localizados**
```
Erro: Could not find required columns in table
Causa: Estrutura da pÃ¡gina mudou
SoluÃ§Ã£o: Verificar seletores em config.py / Atualizar cÃ³digo
```

#### 3. Erros de Rede

**Bloqueio do site (HTTP 423)**
```
Erro: Server responded with status 423
Causa: Site bloqueou requisiÃ§Ãµes (rate limiting)
SoluÃ§Ã£o: Aumentar SLEEP_BETWEEN_REQUESTS / Executar em outro horÃ¡rio
```

**Sem conexÃ£o**
```
Erro: Connection refused
Causa: Sem internet ou site fora do ar
SoluÃ§Ã£o: Verificar conexÃ£o / Aguardar site voltar
```

### Sistema de Logs

#### NÃ­veis de Log

```python
logging.INFO    # OperaÃ§Ãµes normais
logging.WARNING # Avisos (nÃ£o impedem execuÃ§Ã£o)
logging.ERROR   # Erros (podem impedir extraÃ§Ã£o de um CNPJ)
```

#### Formato dos Logs

```
2025-10-23 11:03:41,543 - INFO - ANBIMA Fund Data Scraper Started
2025-10-23 11:03:41,611 - INFO - Found 2 CNPJs to process
2025-10-23 11:04:32,934 - INFO - âœ“ Successfully scraped data for 48.330.198/0001-06
2025-10-23 11:05:22,121 - INFO - ANBIMA Fund Data Scraper Completed Successfully
```

#### LocalizaÃ§Ã£o dos Logs

```
logs/
â””â”€â”€ scraper_YYYYMMDD_HHMMSS.log
```

---

## ğŸ” SoluÃ§Ã£o de Problemas

### Problemas Comuns

#### 1. Scraper nÃ£o encontra elementos

**Sintomas:**
- Erros de "element not found"
- Timeouts constantes

**DiagnÃ³stico:**
```bash
python3 main.py --no-headless
```
Observe visualmente o que estÃ¡ acontecendo

**SoluÃ§Ãµes:**
1. Verificar se site mudou estrutura
2. Aumentar timeouts em `config.py`
3. Limpar cache do navegador:
```bash
rm -rf ~/.wdm/
```

#### 2. Resultados incompletos

**Sintomas:**
- Menos registros que esperado
- Dados faltando

**DiagnÃ³stico:**
Verificar logs em `logs/scraper_*.log`

**SoluÃ§Ãµes:**
1. Aumentar `SLEEP_BETWEEN_REQUESTS`
2. Verificar conexÃ£o com internet
3. Executar em horÃ¡rio de menor trÃ¡fego

#### 3. Performance lenta

**Sintomas:**
- Scraper muito lento
- Timeouts frequentes

**SoluÃ§Ãµes:**
1. Verificar velocidade da internet
2. Fechar outras aplicaÃ§Ãµes
3. Reduzir nÃºmero de CNPJs por execuÃ§Ã£o
4. Usar modo headless (mais rÃ¡pido)

#### 4. Erros de memÃ³ria

**Sintomas:**
- Scraper trava
- Sistema congela

**SoluÃ§Ãµes:**
1. Processar CNPJs em lotes menores
2. Aumentar swap/memÃ³ria virtual
3. Fechar outras aplicaÃ§Ãµes

### Debugging AvanÃ§ado

#### Habilitar Logs Detalhados do Selenium

```python
# Em config.py, adicionar:
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Salvar Screenshots

```python
# Em anbima_scraper.py, adicionar:
driver.save_screenshot('debug_screenshot.png')
```

#### Salvar HTML da PÃ¡gina

```python
# Em anbima_scraper.py, adicionar:
with open('page_source.html', 'w') as f:
    f.write(driver.page_source)
```

---

## â“ Perguntas Frequentes

### Geral

**P: O scraper funciona em Windows?**
R: Sim, funciona em Windows, macOS e Linux.

**P: Preciso de conhecimentos de programaÃ§Ã£o para usar?**
R: NÃ£o, basta seguir o guia de instalaÃ§Ã£o e uso.

**P: Ã‰ legal fazer scraping do site da ANBIMA?**
R: Os dados sÃ£o pÃºblicos, mas sempre respeite os termos de uso do site.

### Funcionalidades

**P: Quantos CNPJs posso processar de uma vez?**
R: NÃ£o hÃ¡ limite tÃ©cnico, mas recomenda-se lotes de atÃ© 50 para evitar bloqueios.

**P: Posso agendar execuÃ§Ãµes automÃ¡ticas?**
R: Sim, use cron (Linux/Mac) ou Task Scheduler (Windows).

**P: Como obter dados histÃ³ricos de meses anteriores?**
R: O site mostra apenas 22 dias Ãºteis. Para histÃ³rico maior, execute periodicamente e acumule os dados.

**P: Posso extrair outros campos alÃ©m de Data e Valor da cota?**
R: Sim, modifique o cÃ³digo em `anbima_scraper.py` para incluir outros campos.

### TÃ©cnico

**P: Qual navegador Ã© necessÃ¡rio?**
R: Google Chrome. O ChromeDriver Ã© baixado automaticamente.

**P: Posso usar Firefox ou Edge?**
R: Sim, mas precisa modificar o cÃ³digo para usar GeckoDriver ou EdgeDriver.

**P: O scraper funciona com VPN?**
R: Sim, mas pode ser mais lento dependendo da VPN.

**P: Como acelerar o scraper?**
R: Use modo headless e reduza timeouts se sua conexÃ£o for rÃ¡pida.

---

## âš ï¸ LimitaÃ§Ãµes Conhecidas

### LimitaÃ§Ãµes do Site

1. **Dados HistÃ³ricos**: Apenas 22 dias Ãºteis disponÃ­veis
2. **Sem API**: NÃ£o hÃ¡ API oficial da ANBIMA para este tipo de consulta
3. **Rate Limiting**: Site pode bloquear requisiÃ§Ãµes excessivas
4. **Estrutura DinÃ¢mica**: Site pode mudar estrutura HTML sem aviso

### LimitaÃ§Ãµes TÃ©cnicas

1. **DependÃªncia do Chrome**: Requer Google Chrome instalado
2. **JavaScript ObrigatÃ³rio**: Site requer JavaScript habilitado
3. **ConexÃ£o Internet**: NecessÃ¡ria durante toda execuÃ§Ã£o
4. **Performance**: ~50 segundos por fundo em mÃ©dia

### LimitaÃ§Ãµes Funcionais

1. **Sem ValidaÃ§Ã£o de CNPJ**: NÃ£o valida formato antes de consultar
2. **Sem Cache**: Cada execuÃ§Ã£o consulta o site novamente
3. **Sem ParalelizaÃ§Ã£o**: Processa um CNPJ por vez
4. **Sem Interface GrÃ¡fica**: Apenas linha de comando

---

## ğŸ”„ ManutenÃ§Ã£o e AtualizaÃ§Ãµes

### Verificando AtualizaÃ§Ãµes

```bash
# Atualizar dependÃªncias
pip3 install --upgrade -r requirements.txt

# Verificar versÃµes
pip3 list | grep -E "selenium|pandas|openpyxl"
```

### Backup de Dados

**Recomendado fazer backup de:**
- Arquivos de saÃ­da (`output_*.xlsx`)
- Logs (`logs/*.log`)
- Arquivo de entrada (`input_cnpjs.xlsx`)

```bash
# Exemplo de backup
mkdir backup_$(date +%Y%m%d)
cp output_*.xlsx logs/*.log backup_$(date +%Y%m%d)/
```

### AtualizaÃ§Ãµes do Site ANBIMA

Se o site da ANBIMA mudar estrutura:

1. **Verificar seletores** em `config.py`
2. **Atualizar XPath/CSS** conforme necessÃ¡rio
3. **Testar com --no-headless** para debug visual
4. **Consultar logs** para identificar mudanÃ§as

### Roadmap de Melhorias

- [ ] **Interface Web**: Dashboard para executar e visualizar resultados
- [ ] **API REST**: Expor funcionalidades via API
- [ ] **Banco de Dados**: Armazenar histÃ³rico em PostgreSQL/SQLite
- [ ] **ParalelizaÃ§Ã£o**: Processar mÃºltiplos CNPJs simultaneamente
- [ ] **Cache Inteligente**: Evitar consultas duplicadas
- [ ] **NotificaÃ§Ãµes**: Email/Slack ao concluir execuÃ§Ã£o
- [ ] **AnÃ¡lise de Dados**: GrÃ¡ficos e estatÃ­sticas automÃ¡ticas
- [ ] **Docker**: ContainerizaÃ§Ã£o para fÃ¡cil deploy

---

## ğŸ“ Suporte e Contato

### Recursos de Ajuda

1. **DocumentaÃ§Ã£o**: Leia este arquivo completamente
2. **Logs**: Sempre verifique `logs/` para detalhes
3. **Debug Visual**: Use `--no-headless` para observar
4. **FAQ**: Consulte seÃ§Ã£o de Perguntas Frequentes

### Reportando Problemas

Ao reportar problemas, inclua:

1. **VersÃ£o do Python**: `python3 --version`
2. **Sistema Operacional**: Windows/macOS/Linux
3. **Arquivo de Log**: Ãšltimo arquivo em `logs/`
4. **Mensagem de Erro**: Copiar mensagem completa
5. **Passos para Reproduzir**: O que vocÃª fez antes do erro

---

## ğŸ“„ LicenÃ§a e Termos de Uso

### LicenÃ§a

Este projeto Ã© fornecido "como estÃ¡", sem garantias de qualquer tipo.

### Termos de Uso

- âœ… Uso educacional e pesquisa
- âœ… ModificaÃ§Ã£o do cÃ³digo-fonte
- âœ… Uso comercial interno
- âš ï¸ Respeitar termos de uso da ANBIMA
- âš ï¸ NÃ£o sobrecarregar servidores
- âŒ Revenda de dados sem autorizaÃ§Ã£o

### Aviso Legal

Os dados extraÃ­dos sÃ£o de fontes pÃºblicas da ANBIMA. O usuÃ¡rio Ã© responsÃ¡vel pelo uso adequado das informaÃ§Ãµes.

---

## ğŸ“ CrÃ©ditos e Agradecimentos

**Desenvolvido com:**
- Python 3.9+
- Selenium WebDriver
- Pandas
- OpenPyXL

**Inspirado em:**
- Necessidade de automaÃ§Ã£o de coleta de dados financeiros
- Melhores prÃ¡ticas de web scraping
- PadrÃµes de desenvolvimento Python

---

## ğŸ“Š EstatÃ­sticas do Projeto

- **Linhas de CÃ³digo**: ~1.200
- **MÃ³dulos**: 4 principais
- **FunÃ§Ãµes**: 20+
- **Cobertura de Testes**: Manual
- **Tempo MÃ©dio de ExecuÃ§Ã£o**: ~50s por fundo
- **Taxa de Sucesso**: >95% (em condiÃ§Ãµes normais)

---

**VersÃ£o da DocumentaÃ§Ã£o**: 1.0  
**Ãšltima AtualizaÃ§Ã£o**: 23 de Outubro de 2025  
**Compatibilidade**: Python 3.9+

---

**Para mais informaÃ§Ãµes, consulte os comentÃ¡rios no cÃ³digo-fonte.**

