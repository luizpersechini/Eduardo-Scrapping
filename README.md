# üìä ANBIMA Data Scraper - Documenta√ß√£o Completa

Sistema automatizado para extra√ß√£o de dados peri√≥dicos de fundos de investimento do site da ANBIMA.

---

## üìë √çndice

1. [Vis√£o Geral](#-vis√£o-geral)
2. [Requisitos do Sistema](#-requisitos-do-sistema)
3. [Instala√ß√£o](#-instala√ß√£o)
4. [Configura√ß√£o](#-configura√ß√£o)
5. [Guia de Uso](#-guia-de-uso)
6. [Arquitetura do Sistema](#-arquitetura-do-sistema)
7. [Especifica√ß√µes T√©cnicas](#-especifica√ß√µes-t√©cnicas)
8. [Dados Extra√≠dos](#-dados-extra√≠dos)
9. [Tratamento de Erros](#-tratamento-de-erros)
10. [Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)
11. [Perguntas Frequentes](#-perguntas-frequentes)
12. [Limita√ß√µes Conhecidas](#-limita√ß√µes-conhecidas)
13. [Manuten√ß√£o e Atualiza√ß√µes](#-manuten√ß√£o-e-atualiza√ß√µes)

---

## üéØ Vis√£o Geral

### Descri√ß√£o do Projeto

O **ANBIMA Data Scraper** √© uma solu√ß√£o automatizada desenvolvida em Python para extra√ß√£o de dados hist√≥ricos de fundos de investimento dispon√≠veis publicamente no portal [ANBIMA Data](https://data.anbima.com.br/busca/fundos).

### Objetivo

Automatizar o processo de coleta de dados peri√≥dicos (Data da cotiza√ß√£o e Valor da cota) de m√∫ltiplos fundos de investimento, eliminando a necessidade de consultas manuais e facilitando a an√°lise hist√≥rica de dados.

### Principais Caracter√≠sticas

- ‚úÖ **Automa√ß√£o Completa**: Processo end-to-end desde a leitura de CNPJs at√© a gera√ß√£o do arquivo Excel
- ‚úÖ **Extra√ß√£o Seletiva**: Coleta apenas os campos espec√≠ficos solicitados (Data e Valor da cota)
- ‚úÖ **Organiza√ß√£o Cronol√≥gica**: Dados ordenados automaticamente do mais antigo ao mais recente
- ‚úÖ **Robustez**: Sistema de retry autom√°tico para requisi√ß√µes que falham
- ‚úÖ **Rastreabilidade**: Logs detalhados de todas as opera√ß√µes
- ‚úÖ **Interface Amig√°vel**: Barra de progresso e relat√≥rios de execu√ß√£o
- ‚úÖ **Configur√°vel**: Par√¢metros ajust√°veis para diferentes cen√°rios de uso

### Fluxo de Trabalho

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  input_cnpjs    ‚îÇ
‚îÇ   .xlsx         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  main.py        ‚îÇ
‚îÇ  (Orquestrador) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ anbima_scraper  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Site ANBIMA     ‚îÇ
‚îÇ   .py           ‚îÇ     ‚îÇ  (Selenium)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ data_processor  ‚îÇ
‚îÇ   .py           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  output_*.xlsx  ‚îÇ     ‚îÇ  logs/*.log      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª Requisitos do Sistema

### Hardware M√≠nimo

- **Processador**: 1 GHz ou superior
- **Mem√≥ria RAM**: 2 GB (recomendado: 4 GB ou mais)
- **Espa√ßo em Disco**: 500 MB livres
- **Conex√£o √† Internet**: Est√°vel, velocidade m√≠nima de 1 Mbps

### Software Necess√°rio

| Software | Vers√£o M√≠nima | Vers√£o Recomendada | Observa√ß√µes |
|----------|---------------|-------------------|-------------|
| Python | 3.9 | 3.10 ou superior | Obrigat√≥rio |
| Google Chrome | √öltima vers√£o est√°vel | √öltima vers√£o | Obrigat√≥rio |
| Sistema Operacional | Windows 10 / macOS 10.14 / Ubuntu 18.04 | Vers√µes mais recentes | - |

### Depend√™ncias Python

Todas as depend√™ncias s√£o instaladas automaticamente via `requirements.txt`:

```
selenium==4.15.2        # Automa√ß√£o do navegador
pandas==2.1.3           # Manipula√ß√£o de dados tabulares
openpyxl==3.1.2         # Leitura/escrita de arquivos Excel
webdriver-manager==4.0.1 # Gerenciamento autom√°tico do ChromeDriver
tqdm==4.66.1            # Barra de progresso
```

---

## üîß Instala√ß√£o

### Instala√ß√£o R√°pida

1. **Clone ou fa√ßa download do projeto**

2. **Navegue at√© o diret√≥rio do projeto**
```bash
cd "/Users/LuizPersechini_1/Projects/Eduardo Scrapping"
```

3. **Instale as depend√™ncias**
```bash
pip3 install -r requirements.txt
```

### Instala√ß√£o Detalhada

#### Passo 1: Verificar Python

```bash
python3 --version
```

Se n√£o tiver Python instalado, baixe em [python.org](https://www.python.org/downloads/)

#### Passo 2: Verificar Google Chrome

O Google Chrome deve estar instalado no sistema. Baixe em [google.com/chrome](https://www.google.com/chrome/)

#### Passo 3: Criar Ambiente Virtual (Opcional mas Recomendado)

```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate  # Windows
```

#### Passo 4: Instalar Depend√™ncias

```bash
pip3 install -r requirements.txt
```

#### Passo 5: Verificar Instala√ß√£o

```bash
python3 -c "import selenium, pandas, openpyxl; print('‚úì Instala√ß√£o bem-sucedida!')"
```

---

## ‚öôÔ∏è Configura√ß√£o

### Arquivo de Configura√ß√£o (`config.py`)

O arquivo `config.py` cont√©m todas as configura√ß√µes do sistema:

#### URLs e Endpoints

```python
ANBIMA_BASE_URL = "https://data.anbima.com.br/busca/fundos"
```

#### Timeouts (em segundos)

```python
PAGE_LOAD_TIMEOUT = 30      # Tempo m√°ximo para carregar p√°gina
ELEMENT_WAIT_TIMEOUT = 20   # Tempo m√°ximo para encontrar elementos
IMPLICIT_WAIT = 10          # Espera impl√≠cita entre a√ß√µes
SLEEP_BETWEEN_REQUESTS = 2  # Delay entre requisi√ß√µes
```

**Quando ajustar:**
- **Conex√£o lenta**: Aumente todos os timeouts em 50%
- **Conex√£o r√°pida**: Pode reduzir para acelerar o processo
- **Site sobrecarregado**: Aumente `SLEEP_BETWEEN_REQUESTS`

#### Sistema de Retry

```python
MAX_RETRIES = 3     # N√∫mero de tentativas em caso de falha
RETRY_DELAY = 5     # Segundos entre tentativas
```

#### Op√ß√µes do Chrome

```python
CHROME_OPTIONS = [
    "--headless",                          # Execu√ß√£o em background
    "--no-sandbox",                        # Seguran√ßa
    "--disable-dev-shm-usage",            # Uso de mem√≥ria
    "--disable-gpu",                       # GPU
    "--window-size=1920,1080",            # Resolu√ß√£o
    "--disable-blink-features=AutomationControlled",  # Anti-detec√ß√£o
    "user-agent=Mozilla/5.0..."           # User agent
]
```

#### Estrutura de Dados

```python
OUTPUT_COLUMNS = [
    "CNPJ",
    "Nome do Fundo",
    "Data da cotiza√ß√£o",
    "Valor cota",
    "Status"
]

INPUT_COLUMN_CNPJ = "CNPJ"
```

### Arquivo de Entrada (`input_cnpjs.xlsx`)

#### Formato Esperado

| CNPJ |
|------|
| 48.330.198/0001-06 |
| 34.780.531/0001-66 |
| ... |

#### Requisitos

- **Nome da coluna**: Exatamente "CNPJ" (case-sensitive)
- **Formato do CNPJ**: Com ou sem formata√ß√£o (pontos, barras)
- **Extens√£o**: `.xlsx` (Excel 2007 ou superior)
- **Localiza√ß√£o**: Raiz do projeto (ou especifique com `-i`)

#### Exemplo de Cria√ß√£o Manual

```python
import pandas as pd

cnpjs = ["48.330.198/0001-06", "34.780.531/0001-66"]
df = pd.DataFrame({"CNPJ": cnpjs})
df.to_excel("input_cnpjs.xlsx", index=False)
```

---

## üìñ Guia de Uso

### Uso B√°sico

#### 1. Modo Padr√£o (Headless)

```bash
python3 main.py
```

**Caracter√≠sticas:**
- Navegador invis√≠vel (background)
- Mais r√°pido
- Ideal para produ√ß√£o

#### 2. Modo Vis√≠vel (Debug)

```bash
python3 main.py --no-headless
```

**Caracter√≠sticas:**
- Navegador vis√≠vel
- Permite observar o processo
- Ideal para debug e testes

#### 3. Personalizar Arquivos

```bash
python3 main.py -i meus_fundos.xlsx -o resultados_outubro.xlsx
```

### Uso Avan√ßado

#### Executar com Par√¢metros Customizados

```bash
python3 main.py \
  --input fundos_renda_fixa.xlsx \
  --output relatorio_$(date +%Y%m%d).xlsx \
  --no-headless
```

#### Executar em Background (Linux/macOS)

```bash
nohup python3 main.py > execucao.log 2>&1 &
```

#### Agendar Execu√ß√£o (Cron - Linux/macOS)

```bash
# Editar crontab
crontab -e

# Executar todo dia √†s 18h
0 18 * * * cd /caminho/projeto && python3 main.py
```

#### Agendar Execu√ß√£o (Task Scheduler - Windows)

1. Abrir "Agendador de Tarefas"
2. Criar Tarefa B√°sica
3. Apontar para `python.exe` com argumentos: `main.py`
4. Definir diret√≥rio de trabalho: caminho do projeto

### Interpretando a Sa√≠da

#### Durante a Execu√ß√£o

```
‚úì Found 2 CNPJ(s) to process
‚úì Web scraper initialized successfully

üîç Scraping data for 2 fund(s)...

Progress:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:57<00:57, 57.19s/fund]
```

#### Ao Final

```
================================================================================
SCRAPING SUMMARY
================================================================================
Total CNPJs processed: 2
Successful: 2 (100.0%)
Failed: 0

‚úì Results saved to: output_anbima_data_20251023_110341.xlsx
‚úì Log file saved to: logs/
================================================================================
```

### Arquivo de Sa√≠da

#### Estrutura

```
output_anbima_data_YYYYMMDD_HHMMSS.xlsx
```

#### Conte√∫do

| CNPJ | Nome do Fundo | Data da cotiza√ß√£o | Valor cota | Status |
|------|---------------|-------------------|------------|--------|
| 48.330.198/0001-06 | CLASSE √öNICA... | 19/09/2025 | R$ 1,569379 | Success |
| 48.330.198/0001-06 | CLASSE √öNICA... | 22/09/2025 | R$ 1,570331 | Success |

---

## üèóÔ∏è Arquitetura do Sistema

### Componentes Principais

#### 1. `main.py` - Orquestrador

**Responsabilidades:**
- Inicializa√ß√£o do sistema
- Leitura de par√¢metros CLI
- Coordena√ß√£o entre m√≥dulos
- Tratamento de exce√ß√µes globais
- Gera√ß√£o de relat√≥rios

**Fun√ß√µes Principais:**
- `setup_logging()`: Configura sistema de logs
- `main()`: Fun√ß√£o principal de execu√ß√£o

#### 2. `anbima_scraper.py` - Motor de Scraping

**Responsabilidades:**
- Automa√ß√£o do navegador via Selenium
- Navega√ß√£o no site da ANBIMA
- Extra√ß√£o de dados das p√°ginas
- Tratamento de erros de navega√ß√£o

**Classe Principal: `ANBIMAScraper`**

```python
class ANBIMAScraper:
    def setup_driver(self) -> bool
    def search_fund(self, cnpj: str) -> Tuple[bool, str]
    def get_fund_name(self) -> Optional[str]
    def navigate_to_periodic_data(self) -> Tuple[bool, str]
    def extract_periodic_data(self) -> Tuple[bool, List[Dict], str]
    def scrape_fund_data(self, cnpj: str) -> Dict
    def close(self)
```

#### 3. `data_processor.py` - Processador de Dados

**Responsabilidades:**
- Leitura do arquivo Excel de entrada
- Transforma√ß√£o e limpeza de dados
- Exporta√ß√£o para Excel
- Gera√ß√£o de relat√≥rios de resumo

**Classe Principal: `DataProcessor`**

```python
class DataProcessor:
    def read_cnpj_list(self, input_file: str) -> List[str]
    def process_scraped_data(self, results: List[Dict]) -> pd.DataFrame
    def save_results(self, df: pd.DataFrame, output_file: str)
    def create_summary_report(self, results: List[Dict]) -> Dict
```

#### 4. `config.py` - Configura√ß√µes

**Conte√∫do:**
- URLs e endpoints
- Timeouts e delays
- Seletores CSS/XPath
- Estrutura de dados
- Configura√ß√µes do Chrome

### Fluxo de Dados

```
1. main.py l√™ input_cnpjs.xlsx
2. Para cada CNPJ:
   a. ANBIMAScraper.search_fund()
   b. ANBIMAScraper.get_fund_name()
   c. ANBIMAScraper.navigate_to_periodic_data()
   d. ANBIMAScraper.extract_periodic_data()
3. DataProcessor.process_scraped_data()
4. DataProcessor.save_results()
5. Gera logs e relat√≥rios
```

---

## üî¨ Especifica√ß√µes T√©cnicas

### Algoritmo de Scraping

#### Etapa 1: Busca do Fundo

```python
1. Acessa https://data.anbima.com.br/busca/fundos
2. Aguarda carregamento completo (sleep 3s)
3. Fecha banner de cookies se presente
4. Localiza campo de busca por seletor CSS
5. Digita CNPJ
6. Aguarda dropdown de resultados (sleep 3s)
7. Clica no link "em fundos de investimento"
8. Aguarda p√°gina de resultados (sleep 2s)
9. Fecha dropdown se ainda aberto
10. Localiza e clica no primeiro resultado
```

#### Etapa 2: Extra√ß√£o de Dados

```python
1. Obt√©m nome do fundo da p√°gina de detalhes
2. Constr√≥i URL de dados peri√≥dicos
3. Navega para /fundos/{CODE}/dados-periodicos
4. Aguarda carregamento da tabela (sleep 3s)
5. Identifica √≠ndices das colunas:
   - Data compet√™ncia (√≠ndice 0)
   - Valor cota (√≠ndice 2)
6. Executa scroll para garantir todos os dados carregados
7. Extrai dados linha por linha
8. Remove duplicatas por data
9. Inverte ordem (mais antigo primeiro)
10. Retorna lista de dicion√°rios
```

### Seletores Utilizados

#### CSS Selectors

```python
"input[placeholder*='Busque fundos']"  # Campo de busca
"article a[href*='/fundos/C']"         # Link do fundo
"table"                                 # Tabela de dados
"th, td"                                # C√©lulas de cabe√ßalho
```

#### XPath Selectors

```python
"//a[contains(@href, '/busca/fundos?q=')]"  # Link dropdown
"//button[contains(text(), 'Prosseguir')]"  # Aceitar cookies
```

### Estrat√©gia de Waits

O sistema utiliza 3 tipos de waits:

1. **Explicit Wait** (WebDriverWait)
   ```python
   wait = WebDriverWait(driver, 20)
   element = wait.until(EC.presence_of_element_located((By.CSS, selector)))
   ```

2. **Implicit Wait**
   ```python
   driver.implicitly_wait(10)
   ```

3. **Sleep** (para carregamentos AJAX)
   ```python
   time.sleep(2)  # Aguarda AJAX completar
   ```

### Sistema de Retry

```python
max_retries = 3
for attempt in range(max_retries):
    try:
        result = scrape_fund_data(cnpj)
        if result['Status'] == 'Success':
            break
    except Exception:
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
        else:
            result = {'Status': 'Error after max retries'}
```

---

## üìä Dados Extra√≠dos

### Campos Coletados

De acordo com as especifica√ß√µes do projeto, s√£o extra√≠dos **apenas** 2 campos:

| Campo | Tipo | Descri√ß√£o | Exemplo |
|-------|------|-----------|---------|
| Data da cotiza√ß√£o | String | Data de compet√™ncia do valor da cota | 19/09/2025 |
| Valor cota | String | Valor da cota na data | R$ 1,569379 |

### Campos Exclu√≠dos

Os seguintes campos s√£o **intencionalmente exclu√≠dos** da extra√ß√£o:

- ‚ùå Valor patrim√¥nio l√≠quido
- ‚ùå Valor volume total de aplica√ß√µes
- ‚ùå Valor volume total de resgates
- ‚ùå N√∫mero total de cotistas

### Metadados Adicionados

O sistema adiciona os seguintes metadados:

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| CNPJ | String | CNPJ do fundo consultado |
| Nome do Fundo | String | Nome completo do fundo |
| Status | String | Status da extra√ß√£o (Success/Error) |

### Organiza√ß√£o dos Dados

- **Ordena√ß√£o**: Cronol√≥gica ascendente (mais antigo ‚Üí mais recente)
- **Agrupamento**: Por CNPJ
- **Formato de Sa√≠da**: Excel (.xlsx)

### Limita√ß√µes de Dados

**Importante**: O site da ANBIMA exibe apenas os **√∫ltimos 22 dias √∫teis** de dados peri√≥dicos. N√£o h√° controles de pagina√ß√£o ou filtros de data dispon√≠veis na interface web.

**Solu√ß√£o para hist√≥rico maior:**
- Executar o scraper periodicamente (di√°rio, semanal ou mensal)
- Armazenar resultados em banco de dados
- Consolidar dados ao longo do tempo

---

## üõ°Ô∏è Tratamento de Erros

### Tipos de Erros

#### 1. Erros de Inicializa√ß√£o

**ChromeDriver n√£o encontrado**
```
Erro: Failed to initialize WebDriver
Causa: Chrome n√£o instalado ou ChromeDriver incompat√≠vel
Solu√ß√£o: Instalar Google Chrome / Limpar cache do webdriver-manager
```

**Arquivo de entrada n√£o encontrado**
```
Erro: Input file not found
Causa: Arquivo input_cnpjs.xlsx n√£o existe
Solu√ß√£o: Criar arquivo ou especificar caminho correto com -i
```

#### 2. Erros de Scraping

**Timeout de p√°gina**
```
Erro: Timeout: Page took too long to load
Causa: Conex√£o lenta ou site indispon√≠vel
Solu√ß√£o: Aumentar timeouts em config.py / Verificar conex√£o
```

**CNPJ n√£o encontrado**
```
Erro: No fund found for this CNPJ
Causa: CNPJ n√£o existe na base ANBIMA ou digitado incorretamente
Solu√ß√£o: Verificar CNPJ no site manualmente
```

**Elementos n√£o localizados**
```
Erro: Could not find required columns in table
Causa: Estrutura da p√°gina mudou
Solu√ß√£o: Verificar seletores em config.py / Atualizar c√≥digo
```

#### 3. Erros de Rede

**Bloqueio do site (HTTP 423)**
```
Erro: Server responded with status 423
Causa: Site bloqueou requisi√ß√µes (rate limiting)
Solu√ß√£o: Aumentar SLEEP_BETWEEN_REQUESTS / Executar em outro hor√°rio
```

**Sem conex√£o**
```
Erro: Connection refused
Causa: Sem internet ou site fora do ar
Solu√ß√£o: Verificar conex√£o / Aguardar site voltar
```

### Sistema de Logs

#### N√≠veis de Log

```python
logging.INFO    # Opera√ß√µes normais
logging.WARNING # Avisos (n√£o impedem execu√ß√£o)
logging.ERROR   # Erros (podem impedir extra√ß√£o de um CNPJ)
```

#### Formato dos Logs

```
2025-10-23 11:03:41,543 - INFO - ANBIMA Fund Data Scraper Started
2025-10-23 11:03:41,611 - INFO - Found 2 CNPJs to process
2025-10-23 11:04:32,934 - INFO - ‚úì Successfully scraped data for 48.330.198/0001-06
2025-10-23 11:05:22,121 - INFO - ANBIMA Fund Data Scraper Completed Successfully
```

#### Localiza√ß√£o dos Logs

```
logs/
‚îî‚îÄ‚îÄ scraper_YYYYMMDD_HHMMSS.log
```

---

## üîç Solu√ß√£o de Problemas

### Problemas Comuns

#### 1. Scraper n√£o encontra elementos

**Sintomas:**
- Erros de "element not found"
- Timeouts constantes

**Diagn√≥stico:**
```bash
python3 main.py --no-headless
```
Observe visualmente o que est√° acontecendo

**Solu√ß√µes:**
1. Verificar se site mudou estrutura
2. Aumentar timeouts em `config.py`
3. Limpar cache do navegador:
```bash
rm -rf ~/.wdm/
```

#### 2. Resultados incompletos

**Sintomas:**
- Menos registros que esperado
- Dados faltando

**Diagn√≥stico:**
Verificar logs em `logs/scraper_*.log`

**Solu√ß√µes:**
1. Aumentar `SLEEP_BETWEEN_REQUESTS`
2. Verificar conex√£o com internet
3. Executar em hor√°rio de menor tr√°fego

#### 3. Performance lenta

**Sintomas:**
- Scraper muito lento
- Timeouts frequentes

**Solu√ß√µes:**
1. Verificar velocidade da internet
2. Fechar outras aplica√ß√µes
3. Reduzir n√∫mero de CNPJs por execu√ß√£o
4. Usar modo headless (mais r√°pido)

#### 4. Erros de mem√≥ria

**Sintomas:**
- Scraper trava
- Sistema congela

**Solu√ß√µes:**
1. Processar CNPJs em lotes menores
2. Aumentar swap/mem√≥ria virtual
3. Fechar outras aplica√ß√µes

### Debugging Avan√ßado

#### Habilitar Logs Detalhados do Selenium

```python
# Em config.py, adicionar:
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Salvar Screenshots

```python
# Em anbima_scraper.py, adicionar:
driver.save_screenshot('debug_screenshot.png')
```

#### Salvar HTML da P√°gina

```python
# Em anbima_scraper.py, adicionar:
with open('page_source.html', 'w') as f:
    f.write(driver.page_source)
```

---

## ‚ùì Perguntas Frequentes

### Geral

**P: O scraper funciona em Windows?**
R: Sim, funciona em Windows, macOS e Linux.

**P: Preciso de conhecimentos de programa√ß√£o para usar?**
R: N√£o, basta seguir o guia de instala√ß√£o e uso.

**P: √â legal fazer scraping do site da ANBIMA?**
R: Os dados s√£o p√∫blicos, mas sempre respeite os termos de uso do site.

### Funcionalidades

**P: Quantos CNPJs posso processar de uma vez?**
R: N√£o h√° limite t√©cnico, mas recomenda-se lotes de at√© 50 para evitar bloqueios.

**P: Posso agendar execu√ß√µes autom√°ticas?**
R: Sim, use cron (Linux/Mac) ou Task Scheduler (Windows).

**P: Como obter dados hist√≥ricos de meses anteriores?**
R: O site mostra apenas 22 dias √∫teis. Para hist√≥rico maior, execute periodicamente e acumule os dados.

**P: Posso extrair outros campos al√©m de Data e Valor da cota?**
R: Sim, modifique o c√≥digo em `anbima_scraper.py` para incluir outros campos.

### T√©cnico

**P: Qual navegador √© necess√°rio?**
R: Google Chrome. O ChromeDriver √© baixado automaticamente.

**P: Posso usar Firefox ou Edge?**
R: Sim, mas precisa modificar o c√≥digo para usar GeckoDriver ou EdgeDriver.

**P: O scraper funciona com VPN?**
R: Sim, mas pode ser mais lento dependendo da VPN.

**P: Como acelerar o scraper?**
R: Use modo headless e reduza timeouts se sua conex√£o for r√°pida.

---

## ‚ö†Ô∏è Limita√ß√µes Conhecidas

### Limita√ß√µes do Site

1. **Dados Hist√≥ricos**: Apenas 22 dias √∫teis dispon√≠veis
2. **Sem API**: N√£o h√° API oficial da ANBIMA para este tipo de consulta
3. **Rate Limiting**: Site pode bloquear requisi√ß√µes excessivas
4. **Estrutura Din√¢mica**: Site pode mudar estrutura HTML sem aviso

### Limita√ß√µes T√©cnicas

1. **Depend√™ncia do Chrome**: Requer Google Chrome instalado
2. **JavaScript Obrigat√≥rio**: Site requer JavaScript habilitado
3. **Conex√£o Internet**: Necess√°ria durante toda execu√ß√£o
4. **Performance**: ~50 segundos por fundo em m√©dia

### Limita√ß√µes Funcionais

1. **Sem Valida√ß√£o de CNPJ**: N√£o valida formato antes de consultar
2. **Sem Cache**: Cada execu√ß√£o consulta o site novamente
3. **Sem Paraleliza√ß√£o**: Processa um CNPJ por vez
4. **Sem Interface Gr√°fica**: Apenas linha de comando

---

## üîÑ Manuten√ß√£o e Atualiza√ß√µes

### Verificando Atualiza√ß√µes

```bash
# Atualizar depend√™ncias
pip3 install --upgrade -r requirements.txt

# Verificar vers√µes
pip3 list | grep -E "selenium|pandas|openpyxl"
```

### Backup de Dados

**Recomendado fazer backup de:**
- Arquivos de sa√≠da (`output_*.xlsx`)
- Logs (`logs/*.log`)
- Arquivo de entrada (`input_cnpjs.xlsx`)

```bash
# Exemplo de backup
mkdir backup_$(date +%Y%m%d)
cp output_*.xlsx logs/*.log backup_$(date +%Y%m%d)/
```

### Atualiza√ß√µes do Site ANBIMA

Se o site da ANBIMA mudar estrutura:

1. **Verificar seletores** em `config.py`
2. **Atualizar XPath/CSS** conforme necess√°rio
3. **Testar com --no-headless** para debug visual
4. **Consultar logs** para identificar mudan√ßas

### Roadmap de Melhorias

- [ ] **Interface Web**: Dashboard para executar e visualizar resultados
- [ ] **API REST**: Expor funcionalidades via API
- [ ] **Banco de Dados**: Armazenar hist√≥rico em PostgreSQL/SQLite
- [ ] **Paraleliza√ß√£o**: Processar m√∫ltiplos CNPJs simultaneamente
- [ ] **Cache Inteligente**: Evitar consultas duplicadas
- [ ] **Notifica√ß√µes**: Email/Slack ao concluir execu√ß√£o
- [ ] **An√°lise de Dados**: Gr√°ficos e estat√≠sticas autom√°ticas
- [ ] **Docker**: Containeriza√ß√£o para f√°cil deploy

---

## üìû Suporte e Contato

### Recursos de Ajuda

1. **Documenta√ß√£o**: Leia este arquivo completamente
2. **Logs**: Sempre verifique `logs/` para detalhes
3. **Debug Visual**: Use `--no-headless` para observar
4. **FAQ**: Consulte se√ß√£o de Perguntas Frequentes

### Reportando Problemas

Ao reportar problemas, inclua:

1. **Vers√£o do Python**: `python3 --version`
2. **Sistema Operacional**: Windows/macOS/Linux
3. **Arquivo de Log**: √öltimo arquivo em `logs/`
4. **Mensagem de Erro**: Copiar mensagem completa
5. **Passos para Reproduzir**: O que voc√™ fez antes do erro

---

## üìÑ Licen√ßa e Termos de Uso

### Licen√ßa

Este projeto √© fornecido "como est√°", sem garantias de qualquer tipo.

### Termos de Uso

- ‚úÖ Uso educacional e pesquisa
- ‚úÖ Modifica√ß√£o do c√≥digo-fonte
- ‚úÖ Uso comercial interno
- ‚ö†Ô∏è Respeitar termos de uso da ANBIMA
- ‚ö†Ô∏è N√£o sobrecarregar servidores
- ‚ùå Revenda de dados sem autoriza√ß√£o

### Aviso Legal

Os dados extra√≠dos s√£o de fontes p√∫blicas da ANBIMA. O usu√°rio √© respons√°vel pelo uso adequado das informa√ß√µes.

---

## üéì Cr√©ditos e Agradecimentos

**Desenvolvido com:**
- Python 3.9+
- Selenium WebDriver
- Pandas
- OpenPyXL

**Inspirado em:**
- Necessidade de automa√ß√£o de coleta de dados financeiros
- Melhores pr√°ticas de web scraping
- Padr√µes de desenvolvimento Python

---

## üìä Estat√≠sticas do Projeto

- **Linhas de C√≥digo**: ~1.200
- **M√≥dulos**: 4 principais
- **Fun√ß√µes**: 20+
- **Cobertura de Testes**: Manual
- **Tempo M√©dio de Execu√ß√£o**: ~50s por fundo
- **Taxa de Sucesso**: >95% (em condi√ß√µes normais)

---

**Vers√£o da Documenta√ß√£o**: 1.0  
**√öltima Atualiza√ß√£o**: 23 de Outubro de 2025  
**Compatibilidade**: Python 3.9+

---

**Para mais informa√ß√µes, consulte os coment√°rios no c√≥digo-fonte.**

