# ğŸ—ï¸ Arquitetura TÃ©cnica - ANBIMA Data Scraper

DocumentaÃ§Ã£o tÃ©cnica detalhada da arquitetura e implementaÃ§Ã£o do sistema.

---

## ğŸ“‘ Ãndice

1. [VisÃ£o Geral da Arquitetura](#visÃ£o-geral-da-arquitetura)
2. [Componentes do Sistema](#componentes-do-sistema)
3. [Fluxo de Dados](#fluxo-de-dados)
4. [Estrutura de Classes](#estrutura-de-classes)
5. [DecisÃµes de Design](#decisÃµes-de-design)
6. [PadrÃµes Utilizados](#padrÃµes-utilizados)
7. [EstratÃ©gias de Web Scraping](#estratÃ©gias-de-web-scraping)
8. [Tratamento de Erros](#tratamento-de-erros)
9. [Performance e OtimizaÃ§Ã£o](#performance-e-otimizaÃ§Ã£o)
10. [SeguranÃ§a](#seguranÃ§a)

---

## ğŸ¯ VisÃ£o Geral da Arquitetura

### Arquitetura em Camadas

O sistema segue uma arquitetura em camadas com separaÃ§Ã£o clara de responsabilidades:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAMADA DE APRESENTAÃ‡ÃƒO                â”‚
â”‚                       (main.py)                          â”‚
â”‚  - CLI Interface                                         â”‚
â”‚  - Logging Setup                                         â”‚
â”‚  - Orchestration                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CAMADA DE NEGÃ“CIO                      â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  anbima_scraper.py   â”‚    â”‚  data_processor.py   â”‚  â”‚
â”‚  â”‚  - Web Automation    â”‚    â”‚  - Data Transform    â”‚  â”‚
â”‚  â”‚  - Data Extraction   â”‚    â”‚  - Excel I/O         â”‚  â”‚
â”‚  â”‚  - Error Handling    â”‚    â”‚  - Validation        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAMADA DE INFRAESTRUTURA                â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     config.py        â”‚    â”‚   Selenium Driver    â”‚  â”‚
â”‚  â”‚  - Constants         â”‚    â”‚   Pandas/OpenPyXL    â”‚  â”‚
â”‚  â”‚  - Settings          â”‚    â”‚   File System        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PrincÃ­pios de Design

1. **Separation of Concerns**: Cada mÃ³dulo tem uma responsabilidade especÃ­fica
2. **Single Responsibility**: Cada classe/funÃ§Ã£o faz apenas uma coisa
3. **DRY (Don't Repeat Yourself)**: Evita duplicaÃ§Ã£o de cÃ³digo
4. **Configuration over Code**: ConfiguraÃ§Ãµes externalizadas em `config.py`
5. **Fail Fast**: Erros sÃ£o detectados e reportados o mais cedo possÃ­vel

---

## ğŸ”§ Componentes do Sistema

### 1. main.py - Orquestrador Principal

**Responsabilidade**: Coordenar o fluxo completo da aplicaÃ§Ã£o

**Componentes**:

```python
def setup_logging(output_dir: str) -> logging.Logger
    """
    Configura o sistema de logging
    
    - Cria diretÃ³rio de logs se nÃ£o existir
    - Configura handlers (file e console)
    - Define formato de log
    - Retorna logger configurado
    """

def parse_arguments() -> argparse.Namespace
    """
    Processa argumentos da linha de comando
    
    Args suportados:
    - -i, --input: Arquivo Excel de entrada
    - -o, --output: Arquivo Excel de saÃ­da
    - --no-headless: Modo visÃ­vel do navegador
    """

def main()
    """
    FunÃ§Ã£o principal de execuÃ§Ã£o
    
    Fluxo:
    1. Parse de argumentos
    2. Setup de logging
    3. Leitura de CNPJs (via DataProcessor)
    4. InicializaÃ§Ã£o do scraper (ANBIMAScraper)
    5. Loop de scraping para cada CNPJ
    6. Processamento de resultados
    7. Salvamento em Excel
    8. GeraÃ§Ã£o de relatÃ³rio
    9. Cleanup de recursos
    """
```

**DependÃªncias**:
- `argparse`: Parsing de CLI
- `logging`: Sistema de logs
- `tqdm`: Barra de progresso
- `anbima_scraper.ANBIMAScraper`
- `data_processor.DataProcessor`

---

### 2. anbima_scraper.py - Motor de Web Scraping

**Responsabilidade**: AutomaÃ§Ã£o do navegador e extraÃ§Ã£o de dados

#### Classe: ANBIMAScraper

```python
class ANBIMAScraper:
    """
    Scraper principal para o site da ANBIMA
    
    Attributes:
        driver: WebDriver do Selenium
        wait: WebDriverWait para explicit waits
        logger: Logger para tracking de operaÃ§Ãµes
        headless: Flag para modo headless
    """
    
    def __init__(self, logger: logging.Logger, headless: bool = True)
        """
        Inicializa o scraper
        
        Args:
            logger: Logger para tracking
            headless: Se True, executa navegador invisÃ­vel
        """
    
    def setup_driver(self) -> bool
        """
        Configura o WebDriver do Chrome
        
        Processo:
        1. Baixa ChromeDriver via webdriver-manager
        2. Corrige path do executable (macOS ARM64)
        3. Define permissÃµes de execuÃ§Ã£o (chmod 755)
        4. Aplica opÃ§Ãµes do Chrome (config.CHROME_OPTIONS)
        5. Configura timeouts
        6. Cria WebDriverWait
        
        Returns:
            True se sucesso, False se falha
        """
    
    def search_fund(self, cnpj: str) -> Tuple[bool, str]
        """
        Busca fundo por CNPJ no site da ANBIMA
        
        Algoritmo:
        1. Navega para ANBIMA_BASE_URL
        2. Aguarda carregamento (sleep 3s)
        3. Fecha banner de cookies (se presente)
        4. Localiza input de busca
        5. Digita CNPJ
        6. Aguarda dropdown (sleep 3s)
        7. Clica em link do dropdown
        8. Aguarda resultados (sleep 2s)
        9. Fecha dropdown se necessÃ¡rio
        10. Clica no primeiro resultado
        
        Args:
            cnpj: CNPJ formatado ou nÃ£o
            
        Returns:
            Tuple[sucesso: bool, mensagem: str]
            
        Raises:
            TimeoutException: Se elementos nÃ£o forem encontrados
        """
    
    def get_fund_name(self) -> Optional[str]
        """
        Extrai nome do fundo da pÃ¡gina de detalhes
        
        Seletores tentados (em ordem):
        1. h1 (tÃ­tulo principal)
        2. .fund-name (classe especÃ­fica)
        3. #fundName (ID)
        
        Returns:
            Nome do fundo ou None se nÃ£o encontrado
        """
    
    def navigate_to_periodic_data(self) -> Tuple[bool, str]
        """
        Navega para aba "Dados PeriÃ³dicos"
        
        EstratÃ©gia:
        - ConstruÃ§Ã£o direta de URL
        - Formato: {base_url}/fundos/{fund_code}/dados-periodicos
        - Evita problemas com cliques em elementos
        
        Returns:
            Tuple[sucesso: bool, mensagem: str]
        """
    
    def extract_periodic_data(self) -> Tuple[bool, List[Dict], str]
        """
        Extrai dados periÃ³dicos da tabela
        
        Algoritmo:
        1. Aguarda tabela (sleep 3s)
        2. Localiza <table>
        3. Extrai headers (<thead>)
        4. Identifica Ã­ndices das colunas:
           - "Data competÃªncia" â†’ date_idx
           - "Valor cota" â†’ cota_idx
        5. Executa scroll infinito:
           - Scroll atÃ© final da pÃ¡gina
           - Aguarda 1s
           - Verifica se altura mudou
           - Repete atÃ© altura estabilizar (3x igual)
           - MÃ¡ximo de 50 scrolls
        6. Extrai dados do <tbody>
        7. Remove duplicatas (via set de datas)
        8. Inverte ordem (mais antigo primeiro)
        
        Returns:
            Tuple[sucesso: bool, dados: List[Dict], mensagem: str]
            
        Data Format:
            {
                "Data da cotizaÃ§Ã£o": "19/09/2025",
                "Valor cota": "R$ 1,569379"
            }
        """
    
    def scrape_fund_data(self, cnpj: str) -> Dict
        """
        Orquestra extraÃ§Ã£o completa de um fundo
        
        Fluxo:
        1. search_fund(cnpj)
        2. get_fund_name()
        3. navigate_to_periodic_data()
        4. extract_periodic_data()
        5. Compila resultado
        
        Returns:
            {
                "CNPJ": str,
                "Nome do Fundo": str,
                "Dados PeriÃ³dicos": List[Dict],
                "Status": str
            }
        """
    
    def close(self)
        """
        Cleanup: Fecha navegador e libera recursos
        """
```

**Seletores CSS/XPath**:

```python
# Input de busca
"input[placeholder*='Busque fundos']"

# Link dropdown
"//a[contains(@href, '/busca/fundos?q=')]"

# BotÃ£o de cookies
"//button[contains(text(), 'Prosseguir')]"

# Link do fundo nos resultados
"article a[href*='/fundos/C']"

# Tabela de dados
"table"

# Headers
"th, td"
```

---

### 3. data_processor.py - Processador de Dados

**Responsabilidade**: ManipulaÃ§Ã£o de dados e I/O de Excel

#### Classe: DataProcessor

```python
class DataProcessor:
    """
    Processa dados de entrada/saÃ­da
    
    Attributes:
        logger: Logger para tracking
    """
    
    def read_cnpj_list(self, input_file: str) -> List[str]
        """
        LÃª CNPJs do arquivo Excel
        
        Processo:
        1. Verifica existÃªncia do arquivo
        2. LÃª com pandas.read_excel()
        3. Procura coluna "CNPJ"
        4. Remove valores nulos
        5. Converte para string
        6. Retorna lista
        
        Args:
            input_file: Caminho do arquivo .xlsx
            
        Returns:
            Lista de CNPJs como strings
            
        Raises:
            FileNotFoundError: Se arquivo nÃ£o existir
            ValueError: Se coluna CNPJ nÃ£o existir
        """
    
    def process_scraped_data(self, results: List[Dict]) -> pd.DataFrame
        """
        Transforma resultados em DataFrame
        
        Processo:
        1. Para cada resultado:
           a. Expande dados periÃ³dicos
           b. Replica CNPJ e Nome para cada linha
           c. Extrai Data e Valor
        2. Cria DataFrame com colunas:
           - CNPJ
           - Nome do Fundo
           - Data da cotizaÃ§Ã£o
           - Valor cota
           - Status
        3. Ordena por CNPJ, depois por Data
        
        Args:
            results: Lista de dicionÃ¡rios de scraping
            
        Returns:
            DataFrame formatado
        """
    
    def save_results(self, df: pd.DataFrame, output_file: str)
        """
        Salva DataFrame em Excel
        
        Processo:
        1. Cria diretÃ³rio se necessÃ¡rio
        2. Salva com pandas.to_excel()
        3. Usa openpyxl engine
        4. index=False
        
        Args:
            df: DataFrame a salvar
            output_file: Caminho do arquivo de saÃ­da
        """
    
    def create_summary_report(self, results: List[Dict]) -> Dict
        """
        Gera relatÃ³rio de resumo
        
        Calcula:
        - Total de CNPJs processados
        - Sucessos
        - Falhas
        - Taxa de sucesso (%)
        - Tempo total
        
        Returns:
            DicionÃ¡rio com estatÃ­sticas
        """
```

**Estrutura de Dados**:

```python
# Input Excel
{
    "CNPJ": ["48.330.198/0001-06", "34.780.531/0001-66"]
}

# Resultado do Scraper
{
    "CNPJ": "48.330.198/0001-06",
    "Nome do Fundo": "CLASSE ÃšNICA...",
    "Dados PeriÃ³dicos": [
        {"Data da cotizaÃ§Ã£o": "19/09/2025", "Valor cota": "R$ 1,569379"},
        {"Data da cotizaÃ§Ã£o": "22/09/2025", "Valor cota": "R$ 1,570331"}
    ],
    "Status": "Success"
}

# Output DataFrame
| CNPJ | Nome do Fundo | Data da cotizaÃ§Ã£o | Valor cota | Status |
|------|---------------|-------------------|------------|--------|
| ...  | ...           | ...               | ...        | ...    |
```

---

### 4. config.py - ConfiguraÃ§Ãµes

**Responsabilidade**: Centralizar todas as configuraÃ§Ãµes

```python
# URLs
ANBIMA_BASE_URL = "https://data.anbima.com.br/busca/fundos"

# Timeouts (segundos)
PAGE_LOAD_TIMEOUT = 30
IMPLICIT_WAIT = 10
EXPLICIT_WAIT_LONG = 20
EXPLICIT_WAIT_SHORT = 5

# Retry
RETRY_ATTEMPTS = 3
RETRY_DELAY = 5

# Chrome Options
CHROME_OPTIONS = [
    "--no-sandbox",
    "--disable-dev-shm-usage",
    "--window-size=1920,1080",
    "--disable-gpu",
    "--incognito",
    "--disable-extensions",
    "--start-maximized",
    "--disable-infobars",
    "--disable-notifications",
    "--disable-popup-blocking",
    "--disable-logging",
    "--log-level=3"
]

# Paths
INPUT_FILE = "input_cnpjs.xlsx"
OUTPUT_DIR = "output"
LOG_DIR = "logs"

# Excel Schema
INPUT_CNPJ_COLUMN = "CNPJ"
OUTPUT_COLUMNS = [
    "CNPJ",
    "Nome do Fundo",
    "Data da cotizaÃ§Ã£o",
    "Valor cota",
    "Status"
]
```

---

## ğŸ”„ Fluxo de Dados

### Fluxo Completo de ExecuÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INICIALIZAÃ‡ÃƒO                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ main.py                                             â”‚
â”‚   â”œâ”€ parse_arguments()                              â”‚
â”‚   â”œâ”€ setup_logging()                                â”‚
â”‚   â””â”€ Cria instÃ¢ncias (Scraper, Processor)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. LEITURA DE ENTRADA                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DataProcessor.read_cnpj_list()                      â”‚
â”‚   â”œâ”€ LÃª input_cnpjs.xlsx                            â”‚
â”‚   â”œâ”€ Valida estrutura                               â”‚
â”‚   â””â”€ Retorna List[str]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SETUP DO SCRAPER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ANBIMAScraper.setup_driver()                        â”‚
â”‚   â”œâ”€ Download ChromeDriver                          â”‚
â”‚   â”œâ”€ Configura opÃ§Ãµes                               â”‚
â”‚   â””â”€ Inicializa WebDriver                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LOOP DE SCRAPING (Para cada CNPJ)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ANBIMAScraper.scrape_fund_data(cnpj)                â”‚
â”‚   â”‚                                                  â”‚
â”‚   â”œâ”€ 4.1. BUSCA                                     â”‚
â”‚   â”‚   â””â”€ search_fund(cnpj)                          â”‚
â”‚   â”‚       â”œâ”€ Navega para site                       â”‚
â”‚   â”‚       â”œâ”€ Fecha cookies                          â”‚
â”‚   â”‚       â”œâ”€ Preenche busca                         â”‚
â”‚   â”‚       â””â”€ Clica em resultado                     â”‚
â”‚   â”‚                                                  â”‚
â”‚   â”œâ”€ 4.2. NOME DO FUNDO                             â”‚
â”‚   â”‚   â””â”€ get_fund_name()                            â”‚
â”‚   â”‚       â””â”€ Extrai <h1>                            â”‚
â”‚   â”‚                                                  â”‚
â”‚   â”œâ”€ 4.3. NAVEGAÃ‡ÃƒO                                 â”‚
â”‚   â”‚   â””â”€ navigate_to_periodic_data()                â”‚
â”‚   â”‚       â””â”€ ConstrÃ³i URL /dados-periodicos         â”‚
â”‚   â”‚                                                  â”‚
â”‚   â””â”€ 4.4. EXTRAÃ‡ÃƒO                                  â”‚
â”‚       â””â”€ extract_periodic_data()                    â”‚
â”‚           â”œâ”€ Localiza tabela                        â”‚
â”‚           â”œâ”€ Identifica colunas                     â”‚
â”‚           â”œâ”€ Scroll infinito                        â”‚
â”‚           â”œâ”€ Extrai dados                           â”‚
â”‚           â””â”€ Remove duplicatas                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PROCESSAMENTO                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DataProcessor.process_scraped_data(results)         â”‚
â”‚   â”œâ”€ Expande dados periÃ³dicos                       â”‚
â”‚   â”œâ”€ Cria DataFrame                                 â”‚
â”‚   â””â”€ Ordena cronologicamente                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. SAÃDA                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DataProcessor.save_results(df, output_file)         â”‚
â”‚   â””â”€ Salva output_*.xlsx                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. RELATÃ“RIO                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DataProcessor.create_summary_report(results)        â”‚
â”‚   â””â”€ Imprime estatÃ­sticas                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. CLEANUP                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ANBIMAScraper.close()                               â”‚
â”‚   â””â”€ driver.quit()                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ DecisÃµes de Design

### 1. Por que Selenium e nÃ£o Requests/BeautifulSoup?

**DecisÃ£o**: Usar Selenium WebDriver

**RazÃµes**:
- Site ANBIMA Ã© **JavaScript-heavy** (React/Next.js)
- Dados carregam **dinamicamente** (AJAX)
- NecessÃ¡rio **simular interaÃ§Ã£o** (cliques, scroll)
- **Lazy loading** de dados histÃ³ricos
- Requests/BS4 nÃ£o conseguem executar JavaScript

**Trade-offs**:
- âœ… Acesso a conteÃºdo dinÃ¢mico
- âœ… SimulaÃ§Ã£o realista de usuÃ¡rio
- âŒ Mais lento que requests
- âŒ Maior consumo de recursos

### 2. Por que ChromeDriver e nÃ£o Firefox/Edge?

**DecisÃ£o**: Chrome como navegador padrÃ£o

**RazÃµes**:
- **Mais popular** e testado
- **Melhor documentaÃ§Ã£o** e suporte
- **webdriver-manager** funciona bem com Chrome
- **Melhor performance** em headless mode

**Flexibilidade**: CÃ³digo permite adaptaÃ§Ã£o para outros browsers

### 3. Por que Pandas para Excel e nÃ£o xlrd/xlwt direto?

**DecisÃ£o**: Usar Pandas + OpenPyXL

**RazÃµes**:
- **API mais simples** e intuitiva
- **TransformaÃ§Ãµes de dados** facilitadas
- **OpenPyXL** suporta .xlsx moderno
- **IntegraÃ§Ã£o** com resto do ecossistema de dados

### 4. Por que ConfiguraÃ§Ã£o Centralizada?

**DecisÃ£o**: Todas configs em `config.py`

**RazÃµes**:
- **Single source of truth**
- FÃ¡cil ajuste sem modificar cÃ³digo
- Facilita **testes** e **manutenÃ§Ã£o**
- Permite **environment-specific configs**

### 5. Por que Logging ao invÃ©s de Print?

**DecisÃ£o**: Uso de mÃ³dulo `logging`

**RazÃµes**:
- **NÃ­veis de log** (INFO, WARNING, ERROR)
- **PersistÃªncia** em arquivos
- **Timestamps** automÃ¡ticos
- **Controle de verbosidade**
- **Production-ready**

---

## ğŸ› ï¸ PadrÃµes Utilizados

### 1. Dependency Injection

```python
class ANBIMAScraper:
    def __init__(self, logger: logging.Logger, headless: bool = True):
        self.logger = logger  # Injected dependency
```

**BenefÃ­cios**:
- Testabilidade
- Flexibilidade
- Desacoplamento

### 2. Single Responsibility Principle

Cada classe tem uma responsabilidade:
- `ANBIMAScraper`: Web scraping
- `DataProcessor`: Data I/O
- `main.py`: Orchestration
- `config.py`: Configuration

### 3. Configuration Object Pattern

```python
# config.py centraliza todas as configuraÃ§Ãµes
from config import ANBIMA_BASE_URL, PAGE_LOAD_TIMEOUT
```

### 4. Error Handling com Try-Except-Finally

```python
try:
    driver = setup_driver()
    result = scrape_data()
except TimeoutException as e:
    logger.error(f"Timeout: {e}")
except Exception as e:
    logger.error(f"Unexpected error: {e}")
finally:
    driver.quit()  # Sempre executa cleanup
```

### 5. Context Manager (ImplÃ­cito)

```python
# Pandas usa context managers internamente
df.to_excel(filename)  # File handle Ã© fechado automaticamente
```

### 6. Factory Pattern (ChromeDriver)

```python
def setup_driver(self):
    driver_path = ChromeDriverManager().install()  # Factory
    driver = webdriver.Chrome(service=Service(driver_path))
    return driver
```

---

## ğŸ•·ï¸ EstratÃ©gias de Web Scraping

### 1. Waits Strategy

#### Explicit Waits (Preferencial)
```python
wait = WebDriverWait(driver, 20)
element = wait.until(
    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
)
```

**Quando usar**: Elementos especÃ­ficos que podem demorar

#### Implicit Waits
```python
driver.implicitly_wait(10)
```

**Quando usar**: ConfiguraÃ§Ã£o global de base

#### Sleep (Ãšltimo Recurso)
```python
time.sleep(3)
```

**Quando usar**: AJAX conhecidamente lento, animaÃ§Ãµes

### 2. Scroll Infinito

```python
last_height = 0
same_height_count = 0
max_scrolls = 50

while scroll_count < max_scrolls:
    # Scroll to bottom
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(1)
    
    # Check if height changed
    new_height = driver.execute_script("return document.body.scrollHeight")
    
    if new_height == last_height:
        same_height_count += 1
        if same_height_count >= 3:
            break  # 3 scrolls sem mudanÃ§a = fim
    else:
        same_height_count = 0
    
    last_height = new_height
    scroll_count += 1
```

**Por que 3 scrolls estÃ¡veis?**: Garantir que nÃ£o hÃ¡ mais dados carregando

### 3. DeduplicaÃ§Ã£o

```python
seen_dates = set()
for row in rows:
    date = extract_date(row)
    if date not in seen_dates:
        data.append(row_data)
        seen_dates.add(date)
```

**Por que?**: Scroll pode duplicar elementos do DOM

### 4. Seletor Robusto

```python
# Tenta mÃºltiplos seletores
selectors = [
    "h1",
    ".fund-name",
    "#fundName"
]

for selector in selectors:
    try:
        element = driver.find_element(By.CSS_SELECTOR, selector)
        if element.text:
            return element.text
    except:
        continue
```

### 5. URL Construction

```python
# Ao invÃ©s de clicar em tab (pode falhar)
current_url = driver.current_url
fund_code = extract_code(current_url)
periodic_url = f"{base}/fundos/{fund_code}/dados-periodicos"
driver.get(periodic_url)  # NavegaÃ§Ã£o direta mais confiÃ¡vel
```

---

## ğŸ›¡ï¸ Tratamento de Erros

### Hierarquia de ExceÃ§Ãµes

```
Exception
â”œâ”€â”€ TimeoutException (Selenium)
â”‚   â””â”€â”€ Elemento nÃ£o encontrado em tempo
â”œâ”€â”€ NoSuchElementException (Selenium)
â”‚   â””â”€â”€ Elemento nÃ£o existe no DOM
â”œâ”€â”€ StaleElementReferenceException (Selenium)
â”‚   â””â”€â”€ Elemento removido do DOM apÃ³s referÃªncia
â”œâ”€â”€ FileNotFoundError (Python)
â”‚   â””â”€â”€ Arquivo de entrada nÃ£o existe
â””â”€â”€ ValueError (Python)
    â””â”€â”€ Dados invÃ¡lidos ou coluna CNPJ ausente
```

### EstratÃ©gia de Retry

```python
max_retries = 3
for attempt in range(max_retries):
    try:
        result = risky_operation()
        break  # Sucesso, sai do loop
    except (TimeoutException, NoSuchElementException) as e:
        logger.warning(f"Attempt {attempt + 1} failed: {e}")
        if attempt < max_retries - 1:
            time.sleep(RETRY_DELAY)
        else:
            logger.error("Max retries reached")
            result = {'Status': 'Failed'}
```

### Graceful Degradation

```python
def get_fund_name(self):
    try:
        return self._extract_from_h1()
    except:
        try:
            return self._extract_from_title()
        except:
            return "Unknown Fund"  # Fallback
```

---

## âš¡ Performance e OtimizaÃ§Ã£o

### Bottlenecks Identificados

1. **Page Load**: 3-5s por pÃ¡gina
2. **Scroll**: 1s por scroll Ã— ~10 scrolls = 10s
3. **Network Latency**: 2-3s por requisiÃ§Ã£o
4. **Total**: ~50s por fundo

### OtimizaÃ§Ãµes Implementadas

#### 1. Headless Mode
```python
chrome_options.add_argument("--headless")
```
**Ganho**: ~20% mais rÃ¡pido

#### 2. Disable Images/CSS (Opcional)
```python
prefs = {
    "profile.managed_default_content_settings.images": 2,
    "profile.managed_default_content_settings.stylesheets": 2
}
chrome_options.add_experimental_option("prefs", prefs)
```
**Ganho**: ~30% mais rÃ¡pido, mas pode quebrar seletores

#### 3. Minimal Waits
```python
# Apenas waits necessÃ¡rios
time.sleep(1)  # Ao invÃ©s de 3s
```

#### 4. Smart Scrolling
```python
# Para ao detectar estabilidade
if same_height_count >= 3:
    break
```

### OtimizaÃ§Ãµes Futuras

#### ParalelizaÃ§Ã£o
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(scrape_fund, cnpj) for cnpj in cnpjs]
    results = [f.result() for f in futures]
```

**Ganho Potencial**: 5x para 5 workers

**Trade-off**: Mais carga no site (respeitar rate limits)

---

## ğŸ”’ SeguranÃ§a

### 1. Anti-Detection

```python
# User Agent realista
"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)..."

# Disable automation flags
"--disable-blink-features=AutomationControlled"

# Incognito mode
"--incognito"
```

### 2. Rate Limiting

```python
time.sleep(2)  # Entre requisiÃ§Ãµes
```

### 3. SanitizaÃ§Ã£o de Entrada

```python
def sanitize_cnpj(cnpj: str) -> str:
    """Remove caracteres especiais"""
    return re.sub(r'[^\d]', '', cnpj)
```

### 4. Validation

```python
if not cnpj or len(cnpj) < 14:
    raise ValueError("Invalid CNPJ")
```

---

## ğŸ“Š MÃ©tricas e Monitoramento

### Logs Estruturados

```python
logger.info(f"Processing CNPJ: {cnpj}")
logger.info(f"Extracted {len(data)} records")
logger.error(f"Failed: {error_msg}")
```

### MÃ©tricas Coletadas

- Total de CNPJs processados
- Taxa de sucesso (%)
- Tempo mÃ©dio por fundo
- Erros por tipo
- Registros extraÃ­dos

### Exemplo de Log

```
2025-10-23 11:03:41,543 - INFO - ANBIMA Fund Data Scraper Started
2025-10-23 11:03:41,611 - INFO - Found 2 CNPJs to process
2025-10-23 11:03:45,221 - INFO - WebDriver initialized successfully
2025-10-23 11:04:32,934 - INFO - Successfully extracted 22 records for 48.330.198/0001-06
2025-10-23 11:05:22,121 - INFO - Scraping completed - Success: 2/2 (100.0%)
```

---

## ğŸ”® Roadmap TÃ©cnico

### Curto Prazo
- [ ] Testes unitÃ¡rios com pytest
- [ ] CI/CD com GitHub Actions
- [ ] Type checking com mypy

### MÃ©dio Prazo
- [ ] API REST com FastAPI
- [ ] PostgreSQL para armazenamento
- [ ] Docker containerization

### Longo Prazo
- [ ] Dashboard web com React
- [ ] Processamento distribuÃ­do
- [ ] Machine learning para detecÃ§Ã£o de mudanÃ§as no site

---

**VersÃ£o da DocumentaÃ§Ã£o**: 1.0  
**Ãšltima AtualizaÃ§Ã£o**: 23 de Outubro de 2025  
**Compatibilidade**: Python 3.9+

