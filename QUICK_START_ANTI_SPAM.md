# Quick Start - Anti-Spam Configuration

## âœ… What Was Done

Phase 1 and 2 anti-spam solutions have been implemented and tested successfully.

## ğŸš€ How to Use

### Basic Usage (Recommended)
```bash
python3 main_parallel.py -i your_input.xlsx --stealth -w 1
```

### All Options
```bash
# Single worker, stealth, headless (PRODUCTION)
python3 main_parallel.py -i input.xlsx --stealth -w 1

# Two workers, stealth (faster, moderate risk)
python3 main_parallel.py -i input.xlsx --stealth -w 2

# Debug mode (see browser)
python3 main_parallel.py -i input.xlsx --stealth -w 1 --no-headless
```

## ğŸ“Š Expected Performance

| CNPJs | Time | Success Rate |
|-------|------|-------------|
| 10    | ~18 min | 95%+ |
| 50    | ~1.5 hours | 95%+ |
| 161   | ~4.7 hours | 95%+ |

## âš™ï¸ What Changed

1. **Stealth mode ON by default** - Uses undetected-chromedriver
2. **Longer delays** - 5-10 seconds between actions (vs 3-7)
3. **Rate limit detection** - Automatically detects and handles blocks
4. **Exponential backoff** - Waits 60s, 120s, 240s if rate limited
5. **Global rate limiter** - Max 15 requests/minute across all workers

## ğŸ” Monitoring

### Check Progress
```bash
# Watch the progress bar
Overall Progress: 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2/3 [03:55<02:05, success=2, failed=0]
```

### Check Logs
```bash
# View latest log
tail -f logs/scraper_parallel_*.log | grep -E "(Success|Failed|Rate)"
```

## âš ï¸ If You Get Rate Limited

1. **Stop immediately** - Ctrl+C to stop scraping
2. **Wait 30 minutes** - Let the rate limit window reset
3. **Restart with more conservative settings:**
```bash
# Edit config.py:
STEALTH_MAX_DELAY = 15.0  # Increase from 10.0
SLEEP_BETWEEN_REQUESTS = 10  # Increase from 5

# Then restart
python3 main_parallel.py -i input.xlsx --stealth -w 1
```

## ğŸ“ˆ Success Indicators

âœ… **Good Signs:**
- "Successfully scraped CNPJ" messages
- No "Rate limit detected!" warnings
- Progress bar showing success count increasing
- Output file has data for scraped funds

âŒ **Bad Signs:**
- Repeated "Rate limit detected!" messages
- Many "Failed to scrape" errors
- Progress bar stuck
- Empty output file

## ğŸ§ª Test First

Before running on full dataset:
```bash
# Create test file with 2-3 CNPJs
python3 -c "
import pandas as pd
df = pd.DataFrame({'CNPJ': ['00.017.024/0001-53', '31.872.495/0001-72']})
df.to_excel('test.xlsx', index=False)
"

# Run test
python3 main_parallel.py -i test.xlsx --stealth -w 1

# Check results
ls -lh output_anbima_data_parallel_*.xlsx
```

## ğŸ“ Support

- **Implementation Details:** See `ANTI_SPAM_SOLUTION_IMPLEMENTATION.md`
- **Logs:** Check `logs/scraper_parallel_*.log`
- **Test Results:** See test output files from 2026-01-14

---

**Status:** âœ… Tested and working (2026-01-14)
**Success Rate:** 100% for valid CNPJs in test
**No rate limiting detected in test run**
